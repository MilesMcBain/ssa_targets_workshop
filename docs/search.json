[
  {
    "objectID": "end.html",
    "href": "end.html",
    "title": "The End",
    "section": "",
    "text": "The End\nThanks for your participation. There’s more to {targets} than we covered here, but I hope you feel on solid enough foundation to take some steps with it.on your own projects.\nMy general advice is:\n\nKeep it simple! Avoid Dynamic Branching and other advanced techiniques until you’re either confident, or they’re absolutely necessary.\nTry to keep your {tarets} plans (_targets.R) really clean and high level. Don’t junk them up with a lot of implementation detail (code not in funcions).\n\nThis retains their value as a review / communication tool.\n\n\nIf you feel able, I would much appreciate your feedback via this short form.\nIf you’re working through the workshop content afterward and get stuck, or spot any problems, feel free to raise an issue on the workshop GitHub repository.",
    "crumbs": [
      "The End"
    ]
  },
  {
    "objectID": "long_vs_wide.html",
    "href": "long_vs_wide.html",
    "title": "Long vs Wide processes",
    "section": "",
    "text": "Long vs Wide processes\nWithin the example project there’s a bothersome little wrinkle. We call a helper function compute_h3_indices_at_resolutions() twice. This function is creating a set of spatial indices for our data. It’s a potentially expensive process on larger data, and ideally one we’d only perform once.\nWe call it:\n\nOnce in wrangle_and_join_weather() as part of the creation of our ‘clean’ dataset\nOnce in plot_species_class_accuracy_hexes() to create a plot of classifier accuracy by hexagon.\n\nWe’re using the testing data for the model with validation metrics at that point and we dropped the spatial index when we created the training data.\n\n\nWhat we could perhaps to instead is:\n\nCompute the spatial induces for our occurrences in a separate dataset\nJoin them only when needed e.g. for the hex-binned plots.\n\nBut why did this wrinkle appear anyway?\nIn a classic staged script workflow datasets goes through a very linear path.\n\nThere’s this kind of unspoken quest to build the perfect-one-true-clean dataset from which all analysis can flow.\nColumns get added and added, rarely removed.\nDatasets get quite wide.\nOften the binding name is reused each time:\n\n\n  the_dataset &lt;-\n    the_dataset |&gt;\n      mutate( # or join, summarise, rbind, cbind etc.\n        # ...\n      )\n\n\nThis style is what I am going to call a ‘long’ process.\n\nWith {targets} long processes probably spend more CPU cycles than necessary. Why? -\n\n\n\nSmall changes to a target possibly trigger a large number of other targets to be rebuilt. Since the target you changed has a huge chain of targets hanging off it.\n\n\n\nInitially when users get started with {targets} there can be tendency to continue to pursue this pattern of long chains of data transformations which are represented as linear sequences of {targets}.\nWith {targets} we have the choice between:\n\nMinimising end to end running time of the plan\n\nAs is usually the aim in the classic workflow\nUsing ‘long’ processes\nMinimising total amount of running time of the plan ever\n\nBy minimising dependencies between targets\nUsing ‘wide’ processes\n\n\n\nWe’ll make our plan a little wider now by refactoring out the spatial index into a separate target.\n\n\nRefactoring Steps\n\nRemove the this code from wrangle_and_join_weather():\n\n\n  occurrences_weather_hexes &lt;-\n    st_as_sf(\n      occurrences_weather,\n      coords = c(\"decimalLongitude\", \"decimalLatitude\"),\n      remove = FALSE,\n      crs = first(occurrences$geodeticDatum)\n    ) |&gt;\n    mutate(\n      compute_h3_indices_at_resolutions(h3_hex_resolutions, geometry)\n    )\n\n  occurrences_weather_hexes\n\nand place it in a function that creates a dataset containing the spatial indices. We’ll need to refactor it a bit further in a minute.\n\ntar_target(\n    occurrences_hexes,\n    create_h3_indexes(\n      occurrences_weather,\n      h3_hex_resolutions\n    )\n  )\n\n\nChange the name of occurrences_weather_hexes to occurrences_weather, since it now has nothing to do with hexes.\n\n\nRemove the h3_hex_resolutions argument from wrangle_and_join_weather()\n\nChange the name also where this dataset is input to other targets\n\n\n\nAdd an id column to occurrences_weather in wrangle_and_join_weather() like:\n\n\noccurrences_weather |&gt;\n  mutate(id = seq(n()))\n\n\nThis will be a key for us to join to.\n\n\nReturn select the id in the select() in create_training_data()\nChange the model formulas in fit_fold_calc_results() and fit_final_species_classification_model() from scientificName ~ . to scientificName ~ . - id to exclude our id.\n\n\nActually changing the model formula in two places highlights we should probably break it out into its own target! Consider that an exercise left to the reader.\n\n\nRefactor create_h3_indexes further to just return id and the h3 indexes:\n\n\n  occurrences_hexes &lt;-\n    st_as_sf(\n      occurrences_weather,\n      coords = c(\"decimalLongitude\", \"decimalLatitude\"),\n      remove = FALSE,\n      crs = first(occurrences_weather$geodeticDatum)\n    ) |&gt;\n    mutate(\n      compute_h3_indices_at_resolutions(h3_hex_resolutions, geometry)\n    ) |&gt; st_drop_geometry() |&gt;\n    select(\n      id,\n      starts_with(\"h3\")\n    )\n\n  occurrences_hexes\n\n\nTo all the targets that start with gg_ and end with hexes pass in our occurrences_hexes target and join to the main dataset before plotting.\n\n\nE.g. in plot_species_distribution_hexes() do this:\n\n\nhex_occurrences &lt;-\n    occurrences_weather  |&gt;\n    left_join(occurrences_hexes, by = \"id\") |&gt; # the new bit\n    st_drop_geometry() |&gt;\n    select(scientificName, h3_hex_8) |&gt;\n    summarise(\n      count = n(),\n      .by = c(\"scientificName\", \"h3_hex_8\")\n    ) |&gt;\n    mutate(\n      geometry = cell_to_polygon(h3_hex_8)\n    ) |&gt;\n    st_as_sf()\n\n  # plot stuff follows\n\n\nIn gg_species_class_accuracy_hexes() remove the h3_hex_resolutions argument and replace with occurrences_hexes.\n\nReplace this code:\n\n  model_validation_predictions_hex &lt;-\n    species_model_validation_data |&gt;\n    st_as_sf(\n      coords = c(\"decimalLongitude\", \"decimalLatitude\"),\n      remove = FALSE,\n      crs = 4326\n    ) |&gt;\n    mutate(\n      compute_h3_indices_at_resolutions(h3_hex_resolutions, geometry)\n    ) |&gt;\n    st_drop_geometry()\n\nWith this:\n\n  model_validation_predictions_hex &lt;-\n    species_model_validation_data |&gt;\n    left_join(occurrences_hexes, by = \"id\")\n\n\nIn plot_species_distributions_points() we no longer have a spatial dataset. So have to make our data spatial for plotting:\n\n\n  occurrences_weather_points &lt;-\n    occurrences_weather |&gt;\n    st_as_sf(\n      coords = c(\"decimalLongitude\", \"decimalLatitude\"),\n      remove = FALSE,\n      crs = first(occurrences_weather$geodeticDatum)\n    )\n\n  p &lt;-\n  ggplot() +\n  geom_sf(\n    data = brisbane_river\n  ) +\n    geom_sf(\n      data = occurrences_weather_points\n    ) +\n    facet_wrap(~scientificName) +\n    theme_light() +\n    theme()\n\n  p\n\n\nWe’re re-converting to point geometry here.\n\nWe could make a similar argument to hexes for another target occurrences_points to be calculated and joined on as needed.\n\nWhere we calculate hexes\nWhere we plot points\n\nAnother exercise for the reader!\n\n\nThe completed refactor is available on this branch of the example project\n\n\nA wider angle\nIf we compare the network graphs before this refactor:\n\n\n\nEverything depending on occurrences_weather_hexes\n\n\nWith the one post this refactor:\n\n\n\nSome targets do not need the hex information\n\n\nWe can see occurrences_weather_hexes is less of a chokepoint, and that the modeling branch of the pipeline no longer depends on the spatial indices.\n\n\nReview\nWhen working with targets {targets} you have a new criteria to optimise for: minimise the lengths of dependency chains.\n\n‘Widening’ your process will lower overall total running time, since you can make best re-use of work saved in the store.\n\nUsing this strategy some things that were annoyingly slow in a linear pipeline are less important to optimise.\n\nCode might be slow, but it hardly ever runs!\nFor {tidyverse} users {targets} enables you to ‘have your cake and eat it too’.\n\nE.g. Why bother with faster packages with more difficult syntax for code that hardly ever runs?",
    "crumbs": [
      "Long vs Wide processes"
    ]
  },
  {
    "objectID": "debugging.html",
    "href": "debugging.html",
    "title": "Debugging {targets} with new access panels",
    "section": "",
    "text": "In her highly recommended talk Object of type ‘closure’ is not subsettable, Jenny Bryan discusses leaving yourself ‘access panels’, like options or arguments that turn on features that help your future self in debugging endeavours. As we shall see {targets} powerful new debugging access panels.\nFirst we look at how problems present and then we review a spectrum of increasingly powerful debugging techniques made available by {targets}\n\n\n\n\nBy default, when an error occurs in targets the pipeline stops. It should be pretty clear from {targets}’ output which target has thrown the error:\n▶ dispatched target species_classification_model\n✖ errored target species_classification_model\n✖ errored pipeline [0.184 seconds]\nError:\n! Error running targets::tar_make()\nError messages: targets::tar_meta(fields = error, complete_only = TRUE)\nDebugging guide: https://books.ropensci.org/targets/debugging.html\nHow to ask for help: https://books.ropensci.org/targets/help.html\nLast error message:\n    I'm broken\nLast error traceback:\n    fit_final_species_classification_model(training_data = training(test_tra...\n    stop(\"I'm broken\")\n    .handleSimpleError(function (condition)  {     state$error &lt;- build_mess...\n    h(simpleError(msg, call))\n\n\n\nIf your problem results in warnings appearing they won’t stop the pipeline. Instead you’ll see something like:\n▶ dispatched target species_classification_model\n● completed target species_classification_model [3.249 seconds]\n✔ skipped target species_model_validation_data\n✔ skipped target base_plot_model_roc_object\n✔ skipped target gg_species_class_accuracy_hexes\n✔ skipped target report\n▶ ended pipeline [5.439 seconds]\nWarning messages:\n1: I'm warning you\n2: 1 targets produced warnings. Run targets::tar_meta(fields = warnings, complete_only = TRUE) for the messages.\nNULL\nSo although we don’t immediately see which target threw the warnings, {targets} does tell us how to find that out. If we run the suggested code:\n\ntargets::tar_meta(fields = warnings, complete_only = TRUE)\n\nwe get precisely the metadata we need:\n# A tibble: 1 × 2\n  name                         warnings\n  &lt;chr&gt;                        &lt;chr&gt;\n1 species_classification_model Im warning you\n\n\n\nIf we just got some nonsense results we might have to work a bit harder to figure out where to start looking for the problem. The process we described for peer reviewing the pipeline in the ‘targets plan’ section is similar to how we could approach finding the logic problem efficiently.\n\n\n\n\n\n\nYou’ll very quickly be able to populate all a target’s inputs in your global environment by using tar_load().\n\nThis is why having functions that use the same argument names as the targets they take as arguments is quite beneficial.\nIf this is not the case you might enjoy loading all the input targets and then calling debugonce, before manually running the problematic target’s expression interactively.\n\n\n\n\nR’s classic can be brought to bear! - Just one obstacle, the targets are typically built in a separate session that we don’t have interactive access to! - We can actually run the pipeline in the current interactive R session. - Just make sure the session is pretty ‘fresh’ or you may create more problems than you solve.\nBy way of example:\n\nput browser() on the first line off fit_final_species_classification_model()\nTo build the pipeline run tar_make(callr_function = NULL)\n\n\nWe’re saying “Don’t use {callr}” which is the method of creating child sessions for our pipeline execution.\n\nEnd up interactively debugging the target:\n✔ skipped target gg_species_distribution_hexes\n✔ skipped target gg_species_distribution_months\n✔ skipped target test_train_split\n✔ skipped target species_classification_model_training_summary\n▶ dispatched target species_classification_model\nCalled from: fit_final_species_classification_model(training_data = training(test_train_split),\n    species_classification_model_training_summary)\nBrowse[1]&gt;\n\n\n\nThis behaves like using browser() above, but is a bit better since you don’t have to make a change to your code that you could forget to undo!\n\nDoes anyone else commit browser() to repos embarrassingly frequently?\n\nIf you add to tar_option_set() in _targets.R\n\ntar_option_set(\n  seed = 2048,\n  debug = \"species_classification_model\"\n)\n\nThen you can call tar_make() and the pipeline will pause for interactive debugging when species_classification_model is reached.\nIf you’d like to speed things up by skipping processing any other targets you can do:\n\ntar_make(species_classification_model, callr_function = NULL, shortcut = TRUE)\n\nAnd {targets} will immediately begin debugging this target.1\nBeing able to name a target to debug increases in usefulness once we understand a more advanced concept called ‘branching’.\n\n\n\nThis is my personal go-to when things just aren’t making sense. A ‘workspace’ is the set of all of a target’s inputs. Since targets should be pure functions, this should be all the state we need to investigate, reproduce, and fix bugs occurring in that target.\nThe first way to use workspaces is to set an option that automatically saves them on error:\n\ntar_option_set(\n  seed = 2048,\n  workspace_on_error = TRUE\n)\n\nWhen an error occurs we will get a slightly different output:\n✔ skipped target test_train_split\n✔ skipped target species_classification_model_training_summary\n▶ dispatched target species_classification_model\n▶ recorded workspace species_classification_model\n✖ errored target species_classification_model\n✖ errored pipeline [0.215 seconds]\nIf we call tar_workspace(species_classification_model), all of the dependencies of species_classification_model will be loaded into the global environment. These are:\n\ntest_train_split\nspecies_classification_model_training_summary\n\nBut isn’t this just the same as calling tar_load?\n\nHopefully / Mostly yes!\nBut occasionally through contrived circumstances you may not be tar_loading what you think you are. In this case there’s no way for that mistake to happen.\nThere are also circumstances where you might not know the names of a specific target’s inputs, and so cannot tar_load them at all.\n\nMore on this when we talk about ‘branching’\n\n\nThere’s also another way to use workspaces, when you might not be getting an error, but you want record a workspace to check on suspicious behaviour. We can instead do:\n\ntar_option_set(\n  seed = 2048,\n  workspaces = c(\"species_classification_model\", \"occurrences_weather_hexes\")\n)\n\nAnd workspaces for these targets will be recorded, whether they error or not.",
    "crumbs": [
      "Debugging {targets} with new access panels"
    ]
  },
  {
    "objectID": "debugging.html#what-it-looks-like-when-things-go-bad",
    "href": "debugging.html#what-it-looks-like-when-things-go-bad",
    "title": "Debugging {targets} with new access panels",
    "section": "",
    "text": "By default, when an error occurs in targets the pipeline stops. It should be pretty clear from {targets}’ output which target has thrown the error:\n▶ dispatched target species_classification_model\n✖ errored target species_classification_model\n✖ errored pipeline [0.184 seconds]\nError:\n! Error running targets::tar_make()\nError messages: targets::tar_meta(fields = error, complete_only = TRUE)\nDebugging guide: https://books.ropensci.org/targets/debugging.html\nHow to ask for help: https://books.ropensci.org/targets/help.html\nLast error message:\n    I'm broken\nLast error traceback:\n    fit_final_species_classification_model(training_data = training(test_tra...\n    stop(\"I'm broken\")\n    .handleSimpleError(function (condition)  {     state$error &lt;- build_mess...\n    h(simpleError(msg, call))\n\n\n\nIf your problem results in warnings appearing they won’t stop the pipeline. Instead you’ll see something like:\n▶ dispatched target species_classification_model\n● completed target species_classification_model [3.249 seconds]\n✔ skipped target species_model_validation_data\n✔ skipped target base_plot_model_roc_object\n✔ skipped target gg_species_class_accuracy_hexes\n✔ skipped target report\n▶ ended pipeline [5.439 seconds]\nWarning messages:\n1: I'm warning you\n2: 1 targets produced warnings. Run targets::tar_meta(fields = warnings, complete_only = TRUE) for the messages.\nNULL\nSo although we don’t immediately see which target threw the warnings, {targets} does tell us how to find that out. If we run the suggested code:\n\ntargets::tar_meta(fields = warnings, complete_only = TRUE)\n\nwe get precisely the metadata we need:\n# A tibble: 1 × 2\n  name                         warnings\n  &lt;chr&gt;                        &lt;chr&gt;\n1 species_classification_model Im warning you\n\n\n\nIf we just got some nonsense results we might have to work a bit harder to figure out where to start looking for the problem. The process we described for peer reviewing the pipeline in the ‘targets plan’ section is similar to how we could approach finding the logic problem efficiently.",
    "crumbs": [
      "Debugging {targets} with new access panels"
    ]
  },
  {
    "objectID": "debugging.html#the-debugging-arsenal",
    "href": "debugging.html#the-debugging-arsenal",
    "title": "Debugging {targets} with new access panels",
    "section": "",
    "text": "You’ll very quickly be able to populate all a target’s inputs in your global environment by using tar_load().\n\nThis is why having functions that use the same argument names as the targets they take as arguments is quite beneficial.\nIf this is not the case you might enjoy loading all the input targets and then calling debugonce, before manually running the problematic target’s expression interactively.\n\n\n\n\nR’s classic can be brought to bear! - Just one obstacle, the targets are typically built in a separate session that we don’t have interactive access to! - We can actually run the pipeline in the current interactive R session. - Just make sure the session is pretty ‘fresh’ or you may create more problems than you solve.\nBy way of example:\n\nput browser() on the first line off fit_final_species_classification_model()\nTo build the pipeline run tar_make(callr_function = NULL)\n\n\nWe’re saying “Don’t use {callr}” which is the method of creating child sessions for our pipeline execution.\n\nEnd up interactively debugging the target:\n✔ skipped target gg_species_distribution_hexes\n✔ skipped target gg_species_distribution_months\n✔ skipped target test_train_split\n✔ skipped target species_classification_model_training_summary\n▶ dispatched target species_classification_model\nCalled from: fit_final_species_classification_model(training_data = training(test_train_split),\n    species_classification_model_training_summary)\nBrowse[1]&gt;\n\n\n\nThis behaves like using browser() above, but is a bit better since you don’t have to make a change to your code that you could forget to undo!\n\nDoes anyone else commit browser() to repos embarrassingly frequently?\n\nIf you add to tar_option_set() in _targets.R\n\ntar_option_set(\n  seed = 2048,\n  debug = \"species_classification_model\"\n)\n\nThen you can call tar_make() and the pipeline will pause for interactive debugging when species_classification_model is reached.\nIf you’d like to speed things up by skipping processing any other targets you can do:\n\ntar_make(species_classification_model, callr_function = NULL, shortcut = TRUE)\n\nAnd {targets} will immediately begin debugging this target.1\nBeing able to name a target to debug increases in usefulness once we understand a more advanced concept called ‘branching’.\n\n\n\nThis is my personal go-to when things just aren’t making sense. A ‘workspace’ is the set of all of a target’s inputs. Since targets should be pure functions, this should be all the state we need to investigate, reproduce, and fix bugs occurring in that target.\nThe first way to use workspaces is to set an option that automatically saves them on error:\n\ntar_option_set(\n  seed = 2048,\n  workspace_on_error = TRUE\n)\n\nWhen an error occurs we will get a slightly different output:\n✔ skipped target test_train_split\n✔ skipped target species_classification_model_training_summary\n▶ dispatched target species_classification_model\n▶ recorded workspace species_classification_model\n✖ errored target species_classification_model\n✖ errored pipeline [0.215 seconds]\nIf we call tar_workspace(species_classification_model), all of the dependencies of species_classification_model will be loaded into the global environment. These are:\n\ntest_train_split\nspecies_classification_model_training_summary\n\nBut isn’t this just the same as calling tar_load?\n\nHopefully / Mostly yes!\nBut occasionally through contrived circumstances you may not be tar_loading what you think you are. In this case there’s no way for that mistake to happen.\nThere are also circumstances where you might not know the names of a specific target’s inputs, and so cannot tar_load them at all.\n\nMore on this when we talk about ‘branching’\n\n\nThere’s also another way to use workspaces, when you might not be getting an error, but you want record a workspace to check on suspicious behaviour. We can instead do:\n\ntar_option_set(\n  seed = 2048,\n  workspaces = c(\"species_classification_model\", \"occurrences_weather_hexes\")\n)\n\nAnd workspaces for these targets will be recorded, whether they error or not.",
    "crumbs": [
      "Debugging {targets} with new access panels"
    ]
  },
  {
    "objectID": "debugging.html#footnotes",
    "href": "debugging.html#footnotes",
    "title": "Debugging {targets} with new access panels",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt may be tempting to use shortcut more frequently to speed things up, but using shortcut is equivalent to running a numbered pipeline stage script without running the prior scripts in the ‘classic R project’ we started with. Do it too often and you’ll have reproducibility debt that needs to be paid down in bulk.↩︎",
    "crumbs": [
      "Debugging {targets} with new access panels"
    ]
  },
  {
    "objectID": "pure_functions.html",
    "href": "pure_functions.html",
    "title": "Pure Functions as units of work",
    "section": "",
    "text": "Functions are fun!\n\noperate_on_a_b &lt;- function(input_a = 1, input_b = 1, operation = `+`) {\n  operation(input_a, input_b)\n}\n\n# Predict the output\noperate_on_a_b(2, 2)\n\noperate_on_a_b(2, 2, c)\n\noperate_on_a_b(2, 2, rnorm)\n\noperate_on_a_b(2, 2, operate_on_a_b)\n\noperate_on_a_b(2, 2, function(x, y) { paste0(x, y) |&gt; as.numeric() })\n\noperate_on_a_b(2, 2, \\(x, y) union(x, y))\n\n\nAnd R really shoves them in your face e.g. lapply, e.g. labels = in {gglpot2}\n\nCan be intimidating at first.\n\nFunctions underpin {targets}\n\nTake home point: USE MORE FUNCTIONS.\n\nEven if you don’t use {targets} your workflows can probably benefit from using more functions\n\n\nQuestion: What is the level of comfort with writing a function?\n\nHow often do you do it? Daily, weekly, monthly?\n\nQuestion: How do we recognise a good time to create a function?\n\n\n\nWhen to create and use a function\n\n\nThe classic doga is: “When you’ve copy-pasted the same code three times”. Connected to the DRY approach “Don’t Repeat Yourself”.\nWhen you need to make a passage of code less complex. Functions allow us to create new forms of expression that can express solutions in terms that better match our domain. Write more ‘elegant’, readable, and maintainble code.\n\n\n\n\n\n\n\n*win = more maintainable, more easilty debuggable code\nQuestion: Think about the worst code you’ve ever had to debug. What were its features? What made debugging it hard?\n\n\nDebug-resistant code\n\n\nLarge amount of environment ‘state’ that is time consuming to set up to recreate bug\nLarge number of lines of code where bug can hide\n\nStepping through it all is time consuming\n\nLarge number of objects or functions involved\n\nUnderstanding them all is a high cognitive load\n\n\n\nIs anyone familiar with how bushfires are fought?\n\nFirefighters create fire breaks (containment lines) to break up fuel (the bushland) into containment zones. The idea is to keep the fire burning within a contained zone until it consumes all the fuel and burns itself out, or weather conditions become less favourable.\n\nFunctions can be ‘containment zones’ for bugs.\n\nState to recreate bug is limited function’s inputs\nPlaces where bug can hide is limited to within function’s code (sometimes)\n\nCould actually be somewhere else, but you’ve narrowed it down\n\n\nFunctions are communication tools.\n\nNaming a procedure after what it does can be as effective as a comment\nThey provide a navigable hyperlinked structure\nExample: “classic_r_project_/R/compute_h3_indices_at_resolutions.R”",
    "crumbs": [
      "Pure Functions as units of work"
    ]
  },
  {
    "objectID": "pure_functions.html#functions-for-the-win",
    "href": "pure_functions.html#functions-for-the-win",
    "title": "Pure Functions as units of work",
    "section": "",
    "text": "*win = more maintainable, more easilty debuggable code\nQuestion: Think about the worst code you’ve ever had to debug. What were its features? What made debugging it hard?\n\n\nDebug-resistant code\n\n\nLarge amount of environment ‘state’ that is time consuming to set up to recreate bug\nLarge number of lines of code where bug can hide\n\nStepping through it all is time consuming\n\nLarge number of objects or functions involved\n\nUnderstanding them all is a high cognitive load\n\n\n\nIs anyone familiar with how bushfires are fought?\n\nFirefighters create fire breaks (containment lines) to break up fuel (the bushland) into containment zones. The idea is to keep the fire burning within a contained zone until it consumes all the fuel and burns itself out, or weather conditions become less favourable.\n\nFunctions can be ‘containment zones’ for bugs.\n\nState to recreate bug is limited function’s inputs\nPlaces where bug can hide is limited to within function’s code (sometimes)\n\nCould actually be somewhere else, but you’ve narrowed it down\n\n\nFunctions are communication tools.\n\nNaming a procedure after what it does can be as effective as a comment\nThey provide a navigable hyperlinked structure\nExample: “classic_r_project_/R/compute_h3_indices_at_resolutions.R”",
    "crumbs": [
      "Pure Functions as units of work"
    ]
  },
  {
    "objectID": "pure_functions.html#but-how-do-design-functions",
    "href": "pure_functions.html#but-how-do-design-functions",
    "title": "Pure Functions as units of work",
    "section": "But how do design functions?",
    "text": "But how do design functions?\n\nHow to size a function as a unit of work for a {targets} plan?\n\nCode size: Not that much more than a screenful of code\nComplexity: It’s like a paragraph or a subheading. One Main idea.\nIdeally one kind of output\n\nA list of things of the same class is quite normal.\nMultiple distinct results is possible e.g. with a list.\n\nPotentially a code smell that your function is doing too much.\nSometimes a result can be opportunistically efficiently calculated as part of something else… fair enough.",
    "crumbs": [
      "Pure Functions as units of work"
    ]
  },
  {
    "objectID": "pure_functions.html#lets-do-this",
    "href": "pure_functions.html#lets-do-this",
    "title": "Pure Functions as units of work",
    "section": "Let’s do this",
    "text": "Let’s do this\nFollow my lead and we’ll do our first refactor in preparation for {targets} to our project.\n\nRefactoring Steps\n\nMove config and library calls into run.R\n\n\nAll the subsequently created functions are going to be called and wired together in run.R\n\n\nWrap up 01_ into a function that fetches data\nWrap up 02_ into a funciton that wrangles data\nSplit up 03_ into a function for each plot\nSplit up 04_ into:\n\n\nfunction the creates the training data\nfunction that creates test train splits\nfunction that does the model parameter grid search\nfunction that fits final model\nfunction that creates validation data from final model and test set\n\n\nSplit 05_ into one function for each plot\nSwap out all the images used in the Rmd for plot objects.\n\n\nThese are already in the global envirnonment and so can be seen by knitr / rmarkdown during render.\n\nThe completed refactor is on the refactor1 branch of our project\nIn particular pay attention to run.R. - Notice how it is far easier to get a handle on what information our project depends on and where that is used?\n\n\nWorkflow tips\n\n{fnmate} for creating a function defintion from an example call.\n‘Jump to definition’ for jumping to the body of a function from a call site.",
    "crumbs": [
      "Pure Functions as units of work"
    ]
  },
  {
    "objectID": "more.html",
    "href": "more.html",
    "title": "More",
    "section": "",
    "text": "More\nSome additional topics. No content here but happy to discuss if there’s time:\n\nMeta programming with {targets}.\n\nYou can create ‘target factories’: targets that generate more than one target in your plan.\nThese are away to build domain specific abstractions into our plans\nSee: https://wlandau.github.io/targetopia/contributing.html\n\nFor large projects {targets} supports having muliple plans.\n\nI use this a fair bit, and it works well for projects that have separate phases.\nE.g. maybe there’s a phase where you’re building a model, and then there’s a later phase after it’s been ‘in production’ where you analayse the performance. These could be separate plans in the one project.\nBe careful about using it to break up a pipeline such that you revert to the classic ‘script per pipeline stage’ form.\nSee: https://books.ropensci.org/targets/projects.html#multiple-projects",
    "crumbs": [
      "More"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Working Smarter With {targets}",
    "section": "",
    "text": "Introduction\nThe objective of this workshop is to change the way you work with R. Rather than sacrificing the fluidity and immediacy of R’s REPL-based programming in the name of ‘reproducibility’, we will find there is a middle way that lets us have both. That way is the {targets} way.\nWe start by setting some context:\n\nWhat brought you to this workshop?\nHave you had any experience with {targets}?\nWhat problems do you have that you hope {targets} can solve?\nDo you anticipate any barriers to moving forward with {targets} in your workplace?\n\n\n\nWhat is {targets}?\n{targets} is a framemork created by Will Landau for building data science pipelines with R. It is part of a growing niche of ‘data orchestration’ tools that conceive of data processing pipelines as graph structures. What sets {targets} apart from the rest is its incredible ergonomics and extensibility facilitated by features of the R programming language.\nYou can view {targets} as a replacement for the classic make tool, which is a famed computation time saver in software development. Make’s most important feature is that it can detect outputs (or ‘targets’) of a software build that have not changed since the previous build, and so can re-use them. This greatly accelerates the development / test cycle by minimising compilation time.\n{targets} shares this feature, but it goes far beyond this, giving the user the ability shape the computational structure of the pipeline so that it can be run optimally within the bounds of resource constraints. Importantly for data science, its features also defeat classes of bugs that affect project reproducibility.\nIn practice the value {targets} delivers as seen by teams is around:\n\nIncreasing the speed of iteration on data science methodology.\nInducing a structure which makes projects more comprehensible, and more easily peer-reviwed.\n\nThe {targets} R package has cleared the high bar set by the rOpenSci peer review process peer review process, and has been accepted on CRAN.\n\n\nOverview of the workshop\n\nTrying to be foundational or like a ‘gentle introduction’. The knowledge you need to get value from {targets} is surprisingly small.\nWe’ll spend time up front understanding the core problems {targets} solves. This will help us articulate the value to our teams.\nOver the course of the workshop we’ll progressively refactor an existing R project, written in a classic style, into a modern {targets} pipeline. This will let us see the benefits accumulate, as we deploy more advanced techniques.\nWe may run out of time, so sections are in priority order. There should be enough instructions to work through the stuff we don’t get to as homework.\n\n\n\nNotation\nIn this workshop material {targets} is used to refer to the R package, while target or targets (no braces) refers to a node in the pipeline graph. We say targets are ‘built’ to refer to executing the code associated with a target to generate its value. The value of target can be any R object. This is a notable difference to make, where targets are files.",
    "crumbs": [
      "Working Smarter With {targets}"
    ]
  },
  {
    "objectID": "typical_R_projects.html",
    "href": "typical_R_projects.html",
    "title": "Strengths and weaknesses of typical R project workflows",
    "section": "",
    "text": "Data goes in, answers, insights, all the magic comes out.\n‘Pipeline’ implies a process which is a kind of linear progression from inputs to outputs.\nContrast this with a process that looks more like a continuous loop, where the aim is to receive input data, react to it, and then rest waiting for the next piece of data.\n\nE.g. a software Application\n\nThe linear aspect is often reflected in how we structure our data analysis projects.",
    "crumbs": [
      "Strengths and weaknesses of typical R project workflows"
    ]
  },
  {
    "objectID": "typical_R_projects.html#script-per-pipeline-stage",
    "href": "typical_R_projects.html#script-per-pipeline-stage",
    "title": "Strengths and weaknesses of typical R project workflows",
    "section": "Script per pipeline ‘stage’",
    "text": "Script per pipeline ‘stage’\nThe most common approach to balancing reproducibility versus other concerns is to break the pipeline up into discrete scripts that map to stages in the linear pipeline. These stages might be conceived of as something like:\n\nAcquire data\nWrangle data\nVisualise data\nModel data\nPresent findings\n\nWith variations as required by context.\nA typical folder structure might look something like:\n .\n├── data\n│   ├── processed_data.Rds\n│   └── raw_data.csv\n├── doc\n│   ├── exploratory_analysis.Rmd\n│   └── report.Rmd\n├── output\n│   ├── insightful_plot.png\n│   └── final_model.Rds\n├── R\n│   └── helpers.R\n├── README.md\n├── run.sh\n└── scripts\n    ├── 01_load_data.R\n    ├── 02_wrangle_data.R\n    ├── 03_visualise_data.R\n    ├── 04_model_data.R\n    └── 05_render_report.R\n\nThere’s a lot of variations on this idea. There might be multiple scripts per phase here, e.g. one per plot (figure)03_visualise_data.R, or one per model. Using more folders seems popular.\nThe key element is that the data analysis is broken down into a series of stages, each of which is captured by a single script file. Quite often these script files are numbered, with it to be implicitly understood that the the correct way to run the pipeline is to run the scripts in numerical order1.\nIf the author is diligent the README.md will contain information about how to run the pipeline, and may provide some kind of run.R or run.sh script which acts as the ‘entry-point’ for kicking off pipeline execution. This is intended to be the thing that you run to reproduce the author’s results.\nThis can be something of a trap because the author likely does not actually use the run.sh script as part of their workflow. - Why would this be so?\n\n\nreasons for not using the ‘run everything’ entry-point.\n\n\nAuthor likely taking advantage of R’s REPL for interactive development. Relies on operating on incomplete pipeline state.\n\nRunning whole pipeline is too slow. Author can’t iterate fast enough if they have to re-run all earlier stages just to make small tweaks to a later stage. E.g. playing with plot presentation\nRunning whole pipeline would have undesirable side-effects like pulling a large amount of data from an API.\n\n\n\nSo the workflow used in practice tends be some combination of: - Interactively run numbered scripts up to the one you want to work on, then manually step through code to create the prerequisite state in the global environment. - Shortcut the early stages of the pipeline by having them write intermediate output files that are read as inputs to later stages. Stages can be worked on independently.\n\nWhen things go stale\nIt’s the real, yet informal, workflow that creates problems.\nIf we used this project structure, and always ran the pipeline from start to finish, we would always know whether our code was in a state consistent with valid outputs 2.\nIf we work interactively, as we inevitably will3, we create opportunities for the pipeline’s code and outputs (be they intermediate or final) to be in conflict. Here’s a few a examples:\n\nChanges are made to 02_wrangle_data.R to support better modeling in 04_model_data.R. We were so keen to write about the improved results, we forgot to re-run 03_visualise_data.R which also outputs some image files that are used in the report.Rmd. When we render report.Rmd it contains images with data that was dropped before running the new model, and our boss is confused as to why we didn’t remove them yet. We look silly.\nIn the midst of running 04_model_data.R interactively we forget that creating a |&gt; chain of data.table transformations can modify the head dataset in-place. We tweak a chain before running it again leading to some columns being transformed twice. When we run the modeling code to completion we get some unexpectedly good results, and save that model for later use in report.Rmd. We write the report out around these results, only to have everything sour at the last minute when we try to run the entire pipeline as one with run.sh and a completely different set of results appears.\n\nBoth of these examples are different aspects of the same core problem. Working interactively with code that can accumulate data i.e. files on disk, data.frames in the global environment, rows in a database, etc. creates the opportunity for the code and the accumulated data to be in an inconsistent state. Sometimes this accumulated data is itself referred to as ‘global state’ or simply ‘state’, and people might say our problem was caused by ‘stale state’, that is: we are working with data that is no longer representative of what our program would output, if we ran it from scratch.\n\n\nCycles vs Lines\n\nThe project structure is strongly linear: Each script is assumed to be fully dependent on those prior, just as each line of code is on the one before.\nOur work pattern is strongly cyclic as we iteratively refine our reasoning, statistical methods, data visualisations etc. This is involves making smaller targeted changes to code at all stages of the pipeline.\n\nThere are forces that we have discussed that exert pressure to avoid running the pipeline in a linear fashion. This creates space for issues:\n\nThe pipeline either fails to run, or gives unexpected results when finally run in a complete pass. Reproducibility fail.\n\nConcepts drift between script files as they are worked on piecemeal. E.g. the same dataset is loaded in multiple script files but referred to by different names. Coherency fail.\n\n\nAs we will see {targets} will remove the pressure to run the entire pipeline end to end, and allow us to work iteratively without the risk of these problems, perhaps faster than ever before.",
    "crumbs": [
      "Strengths and weaknesses of typical R project workflows"
    ]
  },
  {
    "objectID": "typical_R_projects.html#rmd-quarto-monolith",
    "href": "typical_R_projects.html#rmd-quarto-monolith",
    "title": "Strengths and weaknesses of typical R project workflows",
    "section": "Rmd / Quarto Monolith",
    "text": "Rmd / Quarto Monolith\nThe idea of keeping code and output synchronised is often introduced to motivate the use of literate programming tools like Rmarkdown or Quarto. They definitely have something to contribute here, these tools work very well for educational material (like this workshop!), but the format does not scale well to large and complex data science projects.\nThere are two main difficulties:\n\nFundamentally the format is geared toward producing a single output which is a text of some kind. Complex data science projects often have a myriad of other outputs including models, datasets, and other documents. Possibly having your model run binned because pandoc balked at your markdown syntax is not sensible.\nRmarkdown and Quarto offer a caching feature to try to mitigate this but it involves manual cache management, and does not give you control over serialisation formats which mean certain objects will be unable to be restored from cache correctly. It’s up to you to discover which.\nIn projects that involve complex data wrangling or modeling a tension can develop between the text and the code, where the code needs to be complex, but the text is pitched at a different (usually higher) conceptual level. The two fight for the narrative thread, and make for a disjointed / confusing reading experience. I call this illiterate programming.\n\nMy advice is definitely do use Rmarkdown or Quarto, but avoid shoehorning an entire pipeline into the document. Have a pipeline produce the intermediate outputs separately which are then read into the document generation pipeline and given superficial coding treatment e.g. light wrangling into presentation layer plots or tables.",
    "crumbs": [
      "Strengths and weaknesses of typical R project workflows"
    ]
  },
  {
    "objectID": "typical_R_projects.html#footnotes",
    "href": "typical_R_projects.html#footnotes",
    "title": "Strengths and weaknesses of typical R project workflows",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOccasionally this presents refactoring challenges, where a new stage needs to be added late in development and the author might roll with a 02b_ to save updating too many paths.↩︎\nOr would we? How do we decide what valid outputs are? More on this later.↩︎\nProbably one of the reasons you’re using R is the gloriously fluent conversations you can have with your data via the REPL. Working with rapid feedback when you need to simultaneously learn about data and program around it just feels way too good compared to the alternative where you have to wait for a heavy process to spool up and to run each time you have a simple question.↩︎",
    "crumbs": [
      "Strengths and weaknesses of typical R project workflows"
    ]
  },
  {
    "objectID": "targets_plan.html",
    "href": "targets_plan.html",
    "title": "The {targets} plan",
    "section": "",
    "text": "As discussed in the last section, refactoring our project into a collection of functions is actually most of the work in converting a pipeline to use {targets}. The run.R we built already looks a lot like a {targets} ‘plan’.\nIt’s time we actually defined what a {targets} plan is. This can be a little confusing because there are two things that users might refer to as the ‘plan’:\n\nA script file, by convention called _targets.R, that sits in the root folder of the project.\n\n\nSets up environment for the project’s targets. Loads packages, sources script files containing functions to be called. Sets global state: options, environment variables etc.\nReturns, as its last object, a list of target objects.\n\n\nThe list of target objects itself. This data structure is what is analysed to determine the dependency structure of the pipeline graph.\n\nSo in classic R fashion, the definition of the computational graph is itself a data structure which can be manipulated to great metaprogramming effect. For example we can have targets that appear in the _targets.R as a single computational node, but are actually expanded out into several targets in the final returned object.1\nIn this workshop we’ll refer to the _targets.R file as the plan.",
    "crumbs": [
      "The {targets} plan"
    ]
  },
  {
    "objectID": "targets_plan.html#refactoring-steps",
    "href": "targets_plan.html#refactoring-steps",
    "title": "The {targets} plan",
    "section": "Refactoring steps",
    "text": "Refactoring steps\n\nrename run.R to _targets.R\nadd library(targets) and library(tarcheypes) to libraries\n\n\n{tarchetypes} is discussed in the next section\n\n\nreplace set.seed(2048) with tar_option_set(seed = 2048)\nRefactor paths bindings of data files like:\n\n\nweather_data_path &lt;- \"data/brisbane_weather.csv\"\n\n\ntar_file(weather_data_path, \"data/brisbane_weather.csv\")\n\n\nRefactor each object binding like:\n\n\nobject_name &lt;- function_call(arg1, arg2, arg3)\n\nto:\n\ntar_target(\n  object_name,\n  function_call(\n    arg1,\n    arg2,\n    arg3\n  )\n),\n\n\nWrap all the output bindings and their function calls in a list()\n\n\nmake sure there’s a comma separating each\n\n\nReplace render(\"docs/report.Rmd\") with\n\n\ntar_render(report, \"docs/report.Rmd\")\n\n\nWrap each pipeline output in the report in tar_read(), e.g.:\n\n\ntar_read(gg_species_distribution_points)\n\n\nReplace the code to source all files in the R folder with tar_source()\n\nNow we have a {targets} plan!\nThe completed refactor is available on the ‘refactor2’ branch of the R project.",
    "crumbs": [
      "The {targets} plan"
    ]
  },
  {
    "objectID": "targets_plan.html#packages",
    "href": "targets_plan.html#packages",
    "title": "The {targets} plan",
    "section": "Packages",
    "text": "Packages\nThe global environment established by the _targets.R is used to evaluate all targets. This includes loaded packages. In the case of parallelism, as we will see later, the environment is replicated.\nAside from the traditional library() method, there is another way to declare packages, per target, using the packages argument. In this case the packages are loaded right before the target is built, or it’s built object is used in a downstream target. This may be situationally convenient, but because targets that do not share a dependency relationship can be run in any order, this means that packages can be loaded in any order, which could have unexpected consequences. I would suggest avoiding this.\nMany examples you see will set packages globally in _targets.R using tar_option_set(packages = ) this conveys no advantage over the library() method, and has one significant drawback in that renv will not detect these packages automatically as project dependencies.\nMy recommendation is to stick with library()\nThere is one other option related to packages which is important. tar_option_set(imports = ) defines a set of packages whose functions and objects are to be watched for changes. If the package is updated it will cause dependent targets that use updated functions or objects to be rebuilt.\nYou probably don’t want to put every package in here - scanning through large amounts of code slows things down - packages that updated regularly could trigger too much churn in your pipeline - your internal packages or quite unstable packages are probably good candidates.",
    "crumbs": [
      "The {targets} plan"
    ]
  },
  {
    "objectID": "targets_plan.html#constants",
    "href": "targets_plan.html#constants",
    "title": "The {targets} plan",
    "section": "Constants",
    "text": "Constants\nAt the moment we have a objects declared outside the plan. E.g. study_date &lt;- ymd(\"2024-05-08\"). These are still watched for changes, but they are not targets. Their value is not available from the store. My advice is that it makes for a slightly better interactive development and debugging workflows to have them in the list of targets objects, so they become fully fledged targets, and available from the store.\ntarchetypes::tar_plan is a replacement for list()allows targets to be written in the list like:\n\ntar_plan(\n  study_date = ymd(\"2024-05-08\"),\n  tar_target(\n    study_species,\n    search_taxa(c(\"Threskiornis molucca\", \"Threskiornis spinicollis\"))\n  )\n)",
    "crumbs": [
      "The {targets} plan"
    ]
  },
  {
    "objectID": "targets_plan.html#file-targets",
    "href": "targets_plan.html#file-targets",
    "title": "The {targets} plan",
    "section": "File targets",
    "text": "File targets\nIt can be confusing that both input files and output files are declared the same way.\n\nIf you have a file input, and you need targets that depend on the file to be rebuilt if the file changes, use tar_file() in your plan.\nIf you have a target that writes a file, and you want that target to be rebuilt (and file rewritten), if the file is removed, use tar_file() in your plan.\n\nJust remember tar_file() for files!",
    "crumbs": [
      "The {targets} plan"
    ]
  },
  {
    "objectID": "targets_plan.html#footnotes",
    "href": "targets_plan.html#footnotes",
    "title": "The {targets} plan",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMore concretely: We could have a ‘model fit’ target that decomposes into two separate targets for the model and performance statistics. We could use an argument to change the assessment criteria without rebuilding the model. Don’t worry if that didn’t make sense yet.↩︎",
    "crumbs": [
      "The {targets} plan"
    ]
  },
  {
    "objectID": "branching.html",
    "href": "branching.html",
    "title": "Divide and conquer with branching",
    "section": "",
    "text": "The largest chunk of work in our project is our model fitting and the associated grid search. When scaling up these kinds of processes to larger data and larger grids you typically hit some stumbling blocks.\nFor example:\n\nYou ‘add more cores’, or utilise more parallel threads, but then you unexpectedly run out of memory.\nYou get iterations that have model convergence problems due a bad combinations of hyper parameters.\nA colleague suggests you need to expand you hyper-parameter search.\n\nThese types of things are frustrating because the problem might not appear until hours into a very long running process, and the intermediate result of all of those hours is immediately dumped.\nWith {targets} we can use a technique called ‘dynamic branching’ to promote every iteration in a set to its own target.\n\nEach result is individually cached, which means large iterative processes are now resumable.\nWe can also add or remove iterations by changing input data, while reusing the results from previous iterations.\nWe can take advantage of {targets} parallelism features to run the iterations in parallel.\n\nWe’ll refactor the model grid search in our project to use this approach. After we see how it works it’s going to be a little easier to explain why it is called ‘dynamic branching’.",
    "crumbs": [
      "Divide and conquer with branching"
    ]
  },
  {
    "objectID": "branching.html#reusing-existing-grid-search-points",
    "href": "branching.html#reusing-existing-grid-search-points",
    "title": "Divide and conquer with branching",
    "section": "Reusing existing grid search points",
    "text": "Reusing existing grid search points\nNo here’s where if you do a bit of modeling, {targets} should get really exciting.\nFirst let’s explore what happens if we expand the grid search, e.g. by trying a model version with 1000 trees:\n\n  tar_target(\n    num_trees_candidates,\n    c(200, 500, 100, 1000)\n  ),\n\nrunning tar_make() gives:\n...\n✔ skipped branch species_classification_model_training_results_d15e5fdbcf21af3b\n✔ skipped branch species_classification_model_training_results_d19c364718f81c21\n✔ skipped branch species_classification_model_training_results_bde0689544bd8cc7\n▶ dispatched branch species_classification_model_training_results_e8827b8265f7d539\nSetting levels: control = 0, case = 1\nSetting direction: controls &lt; cases\n● completed branch species_classification_model_training_results_e8827b8265f7d539 [0.043 seconds]\n● completed pattern species_classification_model_training_results\n▶ dispatched target species_classification_model_training_summary\n● completed target species_classification_model_training_summary [0.009 seconds]\n▶ dispatched target species_classification_model\n● completed target species_classification_model [0.049 seconds]\n▶ dispatched target species_model_validation_data\n● completed target species_model_validation_data [0.014 seconds]\n✔ skipped target base_plot_model_roc_object\n✔ skipped target gg_species_class_accuracy_hexes\n▶ dispatched target report\n● completed target report [3.984 seconds]\n▶ ended pipeline [5.811 seconds]\nWe can see that:\n\nWe skipped a lot of branches in calculating species_classification_model_training_results\n\nWe only calculated new combinations in our training grid with num_trees = 1000\n\n3 x 15 x 1 of these\n\n\nspecies_classification_model_training_summary changed and since it is an input to species_classification_model the model was refit.\nBUT it turned out that the best model remained the same. So we did not rebuild:\n\nbase_plot_model_roc_object\ngg_species_class_accuracy_hexes\n\nQuestion: How could we refactor the plan if we wanted to make it so that if the best model didn’t change we would not refit final model?\n\n\nRefactoring ideas\n\nWe could make a separate target which is just the first row of species_classification_model_training_summary, which represents the best model.\nThe final model would only be refit if this changes.\n\n\n\nRemember workspaces?\nInitially when I made this refactor I made this mistake:\n\ntar_target(\n    species_classification_model_training_results,\n    fit_fold_calc_results(\n      training_cross_validation_folds$splits,\n      mtry_candidates,\n      num_trees_candidates\n    ),\n    pattern = cross(training_cross_validation_folds, mtry_candidates, num_trees_candidates)\n  )\n\nForgetting that splits was a list column, and so the dataset I want will have an extra layer of list wrapping that needs to be stripped off.\nThe error this generated is hard to debug:\n▶ dispatched target training_cross_validation_folds\n● completed target training_cross_validation_folds [0.007 seconds]\n▶ dispatched branch species_classification_model_training_results_2f9ab41c1360f0ce\n✖ errored branch species_classification_model_training_results_2f9ab41c1360f0ce\n✖ errored pipeline [9.093 seconds]\nError:\n! Error running targets::tar_make()\nError messages: targets::tar_meta(fields = error, complete_only = TRUE)\nDebugging guide: https://books.ropensci.org/targets/debugging.html\nHow to ask for help: https://books.ropensci.org/targets/help.html\nLast error message:\n    No method for objects of class: list\nLast error trace back:\n    fit_fold_calc_results(training_cross_validation_folds$splits,      mtry_...\nPartly because the inputs to species_classification_model_training_results_2f9ab41c1360f0ce are not known exactly. They could be any combination of elements from training_cross_validation_folds, mtry_candidates, and num_trees_candidates. So what would we tar_load() to test the problem interactively?\nThis is the situation we discussed earlier in the context of workspaces. To debug we set:\n\ntar_option_set(\n  seed = 2048,\n  workspace_on_error = TRUE\n)\n\n\nIt’s actually not a bad idea to turn this on defensively when working with dynamic branches.\n\nAn run tar_make():\n▶ dispatched branch species_classification_model_training_results_2f9ab41c1360f0ce\n▶ recorded workspace species_classification_model_training_results_2f9ab41c1360f0ce\n✖ errored branch species_classification_model_training_results_2f9ab41c1360f0ce\n✖ errored pipeline [0.349 seconds]\nand then tar_workspace(species_classification_model_training_results_2f9ab41c1360f0ce).\nWe can now observe that the data object we pass to the fitting function for this branch is:\n&gt; training_cross_validation_folds$splits\n[[1]]\n&lt;Analysis/Assess/Total&gt;\n&lt;5910/1478/7388&gt;\nInside a length 1 list. So when we try to run training() on it inside fit_fold_calc_results() we get this error:\n&gt; training(training_cross_validation_folds$splits)\nError in `training()`:\n! No method for objects of class: list\nRun `rlang::last_trace()` to see where the error occurred.\nSo the quick fix is the [[1]] I added.",
    "crumbs": [
      "Divide and conquer with branching"
    ]
  },
  {
    "objectID": "branching.html#converting-to-parallel",
    "href": "branching.html#converting-to-parallel",
    "title": "Divide and conquer with branching",
    "section": "Converting to parallel",
    "text": "Converting to parallel\nBUT WAIT THERE’S MORE:\nThings run pretty fast now. But what if we wanted to speed things up by making more cores available to run model fits in parallel?\nWe add library(crew) to our packages and then set our options like:\n\n  tar_option_set(\n    seed = 2048,\n    controller = crew_controller_local(workers = 2)\n  )\n\nLet’s blow away our targets store with tar_destroy(), and then run tar_make() to see:\nAn error!\n✖ errored target occurrences\n✖ errored pipeline [3.665 seconds]\nError:\n! Error running targets::tar_make()\nError messages: targets::tar_meta(fields = error, complete_only = TRUE)\nDebugging guide: https://books.ropensci.org/targets/debugging.html\nHow to ask for help: https://books.ropensci.org/targets/help.html\nLast error message:\n    [conflicted] filter found in 2 packages.\nEither pick the one you want with `::`:\n• dplyr::filter\n• stats::filter\nOr declare a preference with `conflicts_prefer()`:\n• `conflicts_prefer(dplyr::filter)`\n• `conflicts_prefer(stats::filter)`\nWhat gives! We called conflicts_prefer at the start of our _targets.R.\nSo in many cases of your {targets} plan can be made parallel with just that one config change. Unfortunately in our case there is a small issue:\n\nThe environment we create in _targets.R is copied to the worker threads that will run targets in parallel\n{targets} Doesn’t reach into package namespaces and copy their internal state. That’s a can of worms!\nSo any package that uses internal state its own namespace for its functionality could have problems when that state is not replicated to workers.\nThis also a problem of calling impure functions!\n\nWe have two packages that utilise internal state in their namespaces:\n\n{conflicted} for the conflict resolution data\n{galah} for its credentials\n\n\nHooks to the rescue\nLuckily this gives us a really good motivating case for something {targets} calls ‘hooks’. Hooks are ways to modify the target definitions in our plan after we have defined them. We have at our disposal:\n\ntar_hook_before(): code to evaluate before a target is built\ntar_hook_inner(): code to wrap around a target any time it appears as a dependency for another target (i.e. in input position)\ntar_hook_outer(): code to wrap around a target after it is built, but before it is saved to the store.\n\nIn our case we can append tar_hook_before() to the end of our list() of target definitions:\n\n  list(\n  # inside list of targets\n  ) |&gt;\n  tar_hook_before(\n    hook = {\n      conflicts_prefer(\n        dplyr::filter,\n      )\n      galah_config(\n        atlas = \"ALA\",\n        email = Sys.getenv(\"ALA_EMAIL\") # You can replace this with your email to run. But you might not want to commit it to a public repository!\n      )\n    }\n  )\n\nHooks can be targeted toward specific targets using the names. A classic use is to strip back {ggplot2} objects before they are saved to the store, since they can hold onto a reference to a large dataset. E.g.\n\nlist(\n  # inside list of targets\n  ) |&gt;\n  tar_hook_outer(\n    hook = lighten_ggplot(.x), # .x is a placeholder we use for the target output\n    names = starts_with(\"gg\")\n  )\n\nThis will postprocess any targets that start their name with “gg”.\n\n\nParallel, finally\nWith the hook in place, we can now build our pipeline in parallel. Exactly how that is done we leave to {targets} and the parallel backend in {crew}. Targets that are not dependent on each other are fair game to run in parallel.\nTargets supports other parallel backends from packages {clustermq} and {future}.\nOne thing that commonly crops up running in parallel is that the increased memory pressure can cause out of memory errors. Luckily with {targets}, we don’t lose work. If you were working on AWS you could change your instance config for more RAM and resume processing.\nThere are helpful options for dealing with resource usage, for example:\n\ntar_option_set(memory = \"transient\") can force targets to be dropped from memory after they’re stored. Under defaults targets can stick around in workers memory. It’s slower to use this but it lowers peak memory usage.\nAlso see storage = \"worker option which can control if the result needs to be copied back to main thread or not.\nWe can also use the deployment option to specify only certain targets go to workers, and the rest get processed in the main thread that runs our plan.\n\nYou don’t need to remember these.\n\nYou can find them in the help for tar_option_set().\nThey can all be set globally or at an individual target level\nThese and other options give you valuable control of ‘shape’ of your pipeline’s process.\nJust remember they exist if you run into resource usage problems.",
    "crumbs": [
      "Divide and conquer with branching"
    ]
  },
  {
    "objectID": "getting_help.html",
    "href": "getting_help.html",
    "title": "Getting Help",
    "section": "",
    "text": "As you branch out and try to do more ambitious things with {targets} you will hit stumbling blocks. Where targets doesn’t behave as you expect.\nCommon themes among these issues are\n\nThings that defeat {targets} static code analysis, so code changes are not detected.\n\nFor example purrr::partial(), purrr::safely(), Vectorize() always return the same function that captures the function you supply in a closure. {targets} static code analysis looks at the body of the function for changes, but not the closure.\n\nObjects store data externally to R\n\ne.g. use external pointers to objects created and managed by compiled C or CPP code.\nlike data.table, stars, raster, terra etc.\ncare needs to be taken to serialise these objects properly. In many cases the default Rds serialisation will fail to reproduce the object, since the loaded object will just have an invalid pointer.\nUse the “format” arg of tar_target() to choose a better format.\nYou can author your own custom formats e.b. {geotargets}\n\n\nThere are things that you likely want to do that aren’t supported in {targets}. It pays to check {tarchetypes} and other ‘targetopia’ packages.\n\nA recurrent request is to have targets that become stale after a certain amount of time passes. E.g. you want to make a new API call if stored target is more than X days old. This feature does not exist explicitly in {targets} but is supported in tarchetypes::tar_age().\n\n\n\n\n\nThe discussions section of the {targets} GitHub repository is a good place to ask questions about how to achieve something with {targets} or why it is not behaving as you expect.\n\nPlease avoid raising these as issues!\n\nThe rOpenSci slack has a dedicated channel to {targets}\nThe #rstats hashtag on Fosstodon / Mastodon is watched by a few {targets} enthusiasts.\n\n\n\n\nhttps://books.ropensci.org/targets/\nIt’s probably the only software manual I have read start to finish.1 - It’s updated frequently. - It’s written for humans. - Not overly dry - Well curated. Doesn’t cover EVERYTHING.",
    "crumbs": [
      "Getting Help"
    ]
  },
  {
    "objectID": "getting_help.html#things-that-can-go-wrong",
    "href": "getting_help.html#things-that-can-go-wrong",
    "title": "Getting Help",
    "section": "",
    "text": "As you branch out and try to do more ambitious things with {targets} you will hit stumbling blocks. Where targets doesn’t behave as you expect.\nCommon themes among these issues are\n\nThings that defeat {targets} static code analysis, so code changes are not detected.\n\nFor example purrr::partial(), purrr::safely(), Vectorize() always return the same function that captures the function you supply in a closure. {targets} static code analysis looks at the body of the function for changes, but not the closure.\n\nObjects store data externally to R\n\ne.g. use external pointers to objects created and managed by compiled C or CPP code.\nlike data.table, stars, raster, terra etc.\ncare needs to be taken to serialise these objects properly. In many cases the default Rds serialisation will fail to reproduce the object, since the loaded object will just have an invalid pointer.\nUse the “format” arg of tar_target() to choose a better format.\nYou can author your own custom formats e.b. {geotargets}\n\n\nThere are things that you likely want to do that aren’t supported in {targets}. It pays to check {tarchetypes} and other ‘targetopia’ packages.\n\nA recurrent request is to have targets that become stale after a certain amount of time passes. E.g. you want to make a new API call if stored target is more than X days old. This feature does not exist explicitly in {targets} but is supported in tarchetypes::tar_age().",
    "crumbs": [
      "Getting Help"
    ]
  },
  {
    "objectID": "getting_help.html#where-to-find-help",
    "href": "getting_help.html#where-to-find-help",
    "title": "Getting Help",
    "section": "",
    "text": "The discussions section of the {targets} GitHub repository is a good place to ask questions about how to achieve something with {targets} or why it is not behaving as you expect.\n\nPlease avoid raising these as issues!\n\nThe rOpenSci slack has a dedicated channel to {targets}\nThe #rstats hashtag on Fosstodon / Mastodon is watched by a few {targets} enthusiasts.",
    "crumbs": [
      "Getting Help"
    ]
  },
  {
    "objectID": "getting_help.html#read-the-fancy-manual",
    "href": "getting_help.html#read-the-fancy-manual",
    "title": "Getting Help",
    "section": "",
    "text": "https://books.ropensci.org/targets/\nIt’s probably the only software manual I have read start to finish.1 - It’s updated frequently. - It’s written for humans. - Not overly dry - Well curated. Doesn’t cover EVERYTHING.",
    "crumbs": [
      "Getting Help"
    ]
  },
  {
    "objectID": "getting_help.html#footnotes",
    "href": "getting_help.html#footnotes",
    "title": "Getting Help",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nApart from those thick glossy concept art drenched jobs that shipped with 90s video game CD-ROMS.↩︎",
    "crumbs": [
      "Getting Help"
    ]
  }
]